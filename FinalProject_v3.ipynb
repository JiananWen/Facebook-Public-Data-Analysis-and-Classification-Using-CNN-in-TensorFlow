{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Public Data Analysis and Classification Using CNN in TensorFlow  \n",
    "INFO 7390---Advances Data Sci/Architecture SEC-04-Spring 2018  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Social media has become a very significant open communication medium currently. This has motivated a lot research on social media data analysis from different aspects using machine learning techniques. Consider the problem of evaluating an Facebook official account, our group frame the problem as collecting dataset and model training. During the progress we extract more than 30,000 pieces of data from 10 public accounts. On the other hands we propose a model combining CNN (convolutional neutral network), correlation and classification to extract\n",
    "features and patterns related social media posts and use them to derive insights about the sources and official account who generated the post with 89.10% accuracy on test dataset. Realize the program’s initial understanding of a public homepage and be able to make further predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chart shows roughly how the data flows in our final project that is implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![my_test_image](Picture/p1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Facebook has a very strict rule among user privacy policy, we can only obtain information published in our own account or information released by official account. Therefore we’ll only focus on public data.\n",
    "Utilizing Facebook graph API services to collect data from different official accounts. The raw data we collected consists of two parts: text and account name. In this part we also need access a token to access graph API. Original dataset, as known as target came serialized in JSON which we acquired by a three-step loop and then performed all exploratory data from this section. In fact, there exist a huge amount of posts are merely forwarded and do not have their own textual information. In this case we forced to jump out of loop to maintain data acquisition.\n",
    "A same method is also used to process pictures. We use google vision API to deal with image content. It provides powerful Image analytics capabilities and enables us to build the next processing. API can see and understand the content within the images and finally return key\n",
    "words which can describe the images forward us.\n",
    "To make data suitable for further classify and analysis, before convolutional neutral network digesting we completed bellowing pre-processing:1.Converted all characters to lower cases.2.Remove punctuations and irrelevant symbols. 3.Remove all blanks via text field. Integrate pre-processed text information into a csv file. This csv file contains label and text and shown as ‘industrial raw material’ in the picture below. Processing through 'factory-CNN'we will reach classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN for Text Classification in TensorFlow\n",
    "\n",
    "In this section we'll talk about the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chart shows roughly how the data flows in our final project that is implemented below.  \n",
    "(by Kim, J. https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![my_test_image](Picture/p2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data From Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import requests\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data by using facebook graph API\n",
    "https://developers.facebook.com/docs/graph-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to use a API token to access to facebook graph API. Read this for how to get token:  \n",
    "https://towardsdatascience.com/how-to-use-facebook-graph-api-and-extract-data-using-python-1839e19d6999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'Your token here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract data from facebook api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reqest_facebook(req):\n",
    "    url = 'https://graph.facebook.com/v2.12/' + req + '?fields=posts.limit(1){picture,message,created_time}&access_token=' + token\n",
    "    r = requests.get(url)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following code will go through all posts of an account and return the text with a lebel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_target(target):\n",
    "    result = reqest_facebook(target).json()\n",
    "\n",
    "    translation = str.maketrans(\"\",\"\", string.punctuation)\n",
    "    try:\n",
    "        #first post\n",
    "        message = result['posts']['data'][0]['message']\n",
    "        message = clean_str(message)\n",
    "        message = message.translate(translation)\n",
    "        time = result['posts']['data'][0]['created_time']\n",
    "        pictureLink = result['posts']['data'][0]['picture']\n",
    "\n",
    "#        print(\"Downloading image\"+str(index))\n",
    "#        pd.download_picture(pictureLink,pic_path)\n",
    "#        print(\"Analysing image\"+str(index))\n",
    "#        picInfo=pp.detect_labels(pic_path)\n",
    "#        print(\"Deleting image\"+str(index))\n",
    "#        os.remove(pic_path)\n",
    "#         print(len(data))\n",
    "        data.append([message,target])\n",
    "    except KeyError as e:\n",
    "        if e.args[0] == \"next\":\n",
    "            print(\"reach the end\")\n",
    "        else:\n",
    "            print(\"connect error, skip this post\")\n",
    "        \n",
    "    #second post\n",
    "    try:\n",
    "        r=requests.get(result['posts']['paging']['next'])\n",
    "        result = r.json()\n",
    "        new_messg = result['data'][0]['message']\n",
    "        new_messg = clean_str(new_messg)\n",
    "        new_messg = new_messg.translate(translation)\n",
    "        new_time = result['data'][0]['created_time']\n",
    "        new_picture = result['data'][0]['picture']\n",
    "\n",
    "#        print(\"Downloading image\"+str(index))\n",
    "#        pd.download_picture(new_picture,pic_path)\n",
    "#        print(\"Analysing image\"+str(index))\n",
    "#        picInfo=pp.detect_labels(pic_path)\n",
    "#        print(\"Deleting image\"+str(index))\n",
    "#        os.remove(pic_path)\n",
    "#         print(len(data))   \n",
    "        data.append([new_messg,target])\n",
    "    except KeyError as e:\n",
    "        if e.args[0] == \"next\":\n",
    "            print(\"reach the end\")\n",
    "        else:\n",
    "            print(\"connect error, skip this post\")\n",
    "            \n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(result['paging']['next'])\n",
    "            result = r.json()\n",
    "            new_messg = result['data'][0]['message']\n",
    "            new_messg = clean_str(new_messg)\n",
    "            new_messg = new_messg.translate(translation)\n",
    "            new_time = result['data'][0]['created_time']\n",
    "            new_picture = result['data'][0]['picture']\n",
    "\n",
    "#            print(\"Downloading image\"+str(index))\n",
    "#            pd.download_picture(new_picture,pic_path)\n",
    "#            print(\"Analysing image\"+str(index))\n",
    "#            picInfo=pp.detect_labels(pic_path)\n",
    "#            print(\"Deleting image\"+str(index))\n",
    "#            os.remove(pic_path)\n",
    "#             print(len(data))\n",
    "            data.append([new_messg,target])\n",
    "            if len(data) > datasize:\n",
    "                print(\"reach the limit\")\n",
    "                print(target + \" load data end!\")\n",
    "                break\n",
    "        except KeyError as e:\n",
    "            if e.args[0] == \"next\":\n",
    "                print(\"reach the end\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"connect error, skip this post\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can read the picture from the post and transform the picture to keywords bu using Google vision API:\n",
    "https://cloud.google.com/vision/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google vision API will process a picture and return a list of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# import os\n",
    "# import csv\n",
    "\n",
    "# from google.cloud import vision\n",
    "# from google.cloud.vision import types\n",
    "\n",
    "    \n",
    "# def detect_labels(pic_path):\n",
    "#     client = vision.ImageAnnotatorClient()\n",
    "\n",
    "#     with io.open(pic_path, 'rb') as image_file:\n",
    "#         content = image_file.read()\n",
    "#     image = types.Image(content=content)\n",
    "\n",
    "#     response = client.label_detection(image=image)\n",
    "#     labels = response.label_annotations\n",
    "    \n",
    "#     result=[]\n",
    "#     for label in labels:\n",
    "#         result.append(label.description)\n",
    "        \n",
    "#     return result\n",
    "\n",
    "# def csv_writer(data, path):\n",
    "#     with open(path, \"w\", newline='') as csv_file:\n",
    "#         writer = csv.writer(csv_file, delimiter=',')\n",
    "#         writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the facebook picture url is in a special format, Google API can not use it directly.\n",
    "So, we will download the picture and prcess it then delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# def download_picture(uri,path):\n",
    "#     with open(path, 'wb') as handle:\n",
    "#             response = requests.get(uri, stream=True)\n",
    "\n",
    "#             if not response.ok:\n",
    "#                 print (response)\n",
    "\n",
    "#             for block in response.iter_content(1024):\n",
    "#                 if not block:\n",
    "#                     break\n",
    "\n",
    "#                 handle.write(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the model wo build on CNN does not need the keyword of picture for now. Besides, processing the picture take a very long time.\n",
    "So, we will keep the picture processing code in our project, but we are not going to use it at this point.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for pre-process text:\n",
    "Clean the post text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    from: https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the data and save it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_facebookdata(target, result_path):\n",
    "    print(target)\n",
    "    shot_target(target)\n",
    "            \n",
    "    myFile = open(result_path, 'w', newline='', encoding='utf-8')\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        writer.writerow(['Text','label'])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(\"Writing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin to run.\n",
    "\n",
    "@datasize: the size of post of every official account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steam\n",
      "reach the limit\n",
      "steam load data end!\n",
      "Writing complete\n",
      "nytimes\n",
      "reach the limit\n",
      "nytimes load data end!\n",
      "Writing complete\n",
      "NASA\n",
      "connect error, skip this post\n",
      "reach the limit\n",
      "NASA load data end!\n",
      "Writing complete\n",
      "linkedin\n",
      "reach the limit\n",
      "linkedin load data end!\n",
      "Writing complete\n",
      "bbcnews\n",
      "reach the limit\n",
      "bbcnews load data end!\n",
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "datasize = 10\n",
    "# pic_path = 'temp/tempPic.jpg'\n",
    "data = []\n",
    "target__list= ['steam', 'nytimes', 'NASA', 'linkedin', 'bbcnews']\n",
    "\n",
    "for accout in target__list:\n",
    "    result_path = 'Data/' + accout + '.csv'\n",
    "    run_facebookdata(accout, result_path)\n",
    "    data.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to run test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steam\n",
      "reach the limit\n",
      "steam load data end!\n",
      "Writing complete\n",
      "nytimes\n",
      "reach the limit\n",
      "nytimes load data end!\n",
      "Writing complete\n",
      "NASA\n",
      "connect error, skip this post\n",
      "reach the limit\n",
      "NASA load data end!\n",
      "Writing complete\n",
      "linkedin\n",
      "reach the limit\n",
      "linkedin load data end!\n",
      "Writing complete\n",
      "bbcnews\n",
      "reach the limit\n",
      "bbcnews load data end!\n",
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "datasize = 10\n",
    "# pic_path = 'temp/tempPic.jpg'\n",
    "data = []\n",
    "target__list= ['steam', 'nytimes', 'NASA', 'linkedin', 'bbcnews']\n",
    "\n",
    "for accout in target__list:\n",
    "    result_path = 'Predict/' + accout + '.csv'\n",
    "    run_facebookdata(accout, result_path)\n",
    "    data.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA.csv     bbcnews.csv  linkedin.csv nytimes.csv  steam.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3559</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>get the hottest ticket of the summer send your...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text label\n",
       "count                                                3559  3559\n",
       "unique                                               3516     1\n",
       "top     get the hottest ticket of the summer send your...  NASA\n",
       "freq                                                    5  3559"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/NASA.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>launching monday  nasa s transiting exoplanet ...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the stars above are rich with planetary compan...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>once launched  our transiting exoplanet survey...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on monday  nasa s transiting exoplanet survey ...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we re live from nasa s kennedy space center on...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label\n",
       "0  launching monday  nasa s transiting exoplanet ...  NASA\n",
       "1  the stars above are rich with planetary compan...  NASA\n",
       "2  once launched  our transiting exoplanet survey...  NASA\n",
       "3  on monday  nasa s transiting exoplanet survey ...  NASA\n",
       "4  we re live from nasa s kennedy space center on...  NASA"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you running on Spark, You will need to add tensorflow library to your cluster:   \n",
    "1.click databricks on the top right.    \n",
    "2.click Libary under New.   \n",
    "3.select Upload Python Egg or PyPI    \n",
    "4.fill \"tensorflow\" in Pypi Name    \n",
    "5.Inatall Libary\n",
    "\n",
    "If you running on local, you can check https://www.tensorflow.org/install/ to install Tensorflow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data. The next step is building a CNN model on tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import csv\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import learn\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to load our data and labels  \n",
    "We have five classes in this demo, so the label will be a 5 digits vector like [0,0,0,0,1] and [0,0,0,1,0].\n",
    "A CNN model with five classes required a long time to train. If you want a quick demo, you can use the load_data_and_labels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 5 classes of data\n",
    "def load_data_and_labels5(data_path):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    x=[]\n",
    "    y=[]\n",
    "    # Load data from files\n",
    "    for subdir, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            file_name = data_path + file\n",
    "            ## check if file is hidden file\n",
    "            if not file.startswith('.'):\n",
    "                with open(file_name) as csvfile:\n",
    "                    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "                    next(readCSV)\n",
    "                    for row in readCSV:\n",
    "                        x.append(row[0])\n",
    "                        if row[1] == 'bbcnews':\n",
    "                            y.append([0,0,0,0,1])\n",
    "                        elif row[1] == 'linkedin':\n",
    "                            y.append([0,0,0,1,0])\n",
    "                        elif row[1] == 'NASA':\n",
    "                            y.append([0,0,1,0,0])\n",
    "                        elif row[1] == 'nytimes':\n",
    "                            y.append([0,1,0,0,0])\n",
    "                        elif row[1] == 'steam':\n",
    "                            y.append([1,0,0,0,0])\t\t\t\t\t\n",
    "    y = array (y)\n",
    "    print('5 Category, Data loaded!')\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 3 classes of data\t\n",
    "def load_data_and_labels3(data_path):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    x=[]\n",
    "    y=[]\n",
    "    # Load data from files\n",
    "    for subdir, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            file_name = data_path+file\n",
    "            print(file_name)\n",
    "            if not file.startswith('.'):\n",
    "                with open(file_name) as csvfile:\n",
    "                    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "                    next(readCSV)\n",
    "                    for row in readCSV:\n",
    "                        if row[1] =='bbcnews' or row[1] =='linkedin' or row[1] =='NASA' :\n",
    "                            x.append(row[0])\n",
    "                            if row[1] == 'bbcnews':\n",
    "                                y.append([0,0,1])\n",
    "                            elif row[1] == 'linkedin':\n",
    "                                y.append([0,1,0])\n",
    "                            elif row[1] == 'NASA':\n",
    "                                y.append([1,0,0])\t\t\t\t\n",
    "    y = array (y)\n",
    "    print('3 Category, Data loaded!')\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config parameter：      \n",
    "act_function: 0 for relu, 1 for elu      \n",
    "cost_function: 0 for softmax_cross_entropy, 1 for sigmoid_cross_entropy_with_logits     \n",
    "gradient: 0 for Adam, 1 for Adagrad   \n",
    "epoches: number of epochs    \n",
    "architecture: embedding dimensions for word2vertor and number of filters for CNN    \n",
    "filter_sizes: size for each filters in CNN   \n",
    "init: initialization config, True for uniform, False for random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change config\n",
    "act_function = 0\n",
    "cost_function = 0\n",
    "epochs=1\n",
    "gradient = 0\n",
    "architecture = [128,128]\n",
    "filter_sizes = \"3,4,5\"\n",
    "init = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "dev_sample_percentage = .1\n",
    "data_path = \"Data/\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = architecture[0]\n",
    "filter_sizes = filter_sizes\n",
    "num_filters = architecture[1]\n",
    "dropout_keep_prob =  0.5\n",
    "l2_reg_lambda = 0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size =  64\n",
    "num_epochs = epochs\n",
    "evaluate_every = 100\n",
    "checkpoint_every = 1\n",
    "num_checkpoints = 5\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data and show the data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Category, Data loaded!\n"
     ]
    }
   ],
   "source": [
    "x_text,y=load_data_and_labels5(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building vocabulary for word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-32b0408cf18a>:3: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/jiananwen/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/jiananwen/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% data for traning, 10% for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 771\n",
      "Train/Dev split: 50/5\n"
     ]
    }
   ],
   "source": [
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model we are using form http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/      \n",
    "This model by DENNY BRITZ is licensed under the Apache License Version 2.0 https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      #Change statement: we add few parameters to change the model config\n",
    "      embedding_size, filter_sizes, num_filters, act_function, cost_function, init, l2_reg_lambda):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "    # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                # Apply nonlinearity\n",
    "                #Change statement: we add act_function to switch between relu and elu\n",
    "                if act_function==0:\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                elif act_function==1:\n",
    "                    h = tf.nn.elu(tf.nn.bias_add(conv, b), name=\"elu\")\n",
    "\n",
    "                # relu: make all negtive number in feature map to 0, leave all postive number\n",
    "\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        #flatting\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                #Change statement: we add uniform to switch between random and uniform\n",
    "                initializer=tf.contrib.layers.xavier_initializer(uniform=init))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "  # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            #Change statement: we add cost_function to switch between softmax_cross_entropy_with_logits and sigmoid_cross_entropy_with_logits\n",
    "            if cost_function == 0:\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            elif cost_function == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method to separate for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to ./Runs/1524249879\n",
      "\n",
      "Train Summaries ./Runs/1524249879/summaries/train\n",
      "\n",
      "Dev summaries ./Runs/1524249879/summaries/dev\n",
      "\n",
      "checkpoint_dir ./Runs/1524249879/checkpoints\n",
      "\n",
      "checkpoint_prefix ./Runs/1524249879/checkpoints/model\n",
      "\n",
      "2018-04-20T14:44:40.458948: step 1, loss 5.6489, acc 0.12\n",
      "Saved model checkpoint to ./Runs/1524249879/checkpoints/model-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        #create cnn using the config \n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            act_function=act_function,\n",
    "            cost_function=cost_function,\n",
    "            init=init,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "          \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        if gradient == 0:\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        elif gradient ==1:\n",
    "            optimizer = tf.train.AdagradOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        \n",
    "         # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.join(os.path.curdir, \"Runs\", timestamp)\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "        \n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        print(\"Train Summaries {}\\n\".format(train_summary_dir))\n",
    "#         dbutils.fs.put(train_summary_dir, tf.summary)\n",
    "#         dbutils.fs.put(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        print(\"Dev summaries {}\\n\".format(dev_summary_dir))\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.join(out_dir, \"checkpoints\")\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        print(\"checkpoint_dir {}\\n\".format(checkpoint_dir))\n",
    "        print(\"checkpoint_prefix {}\\n\".format(checkpoint_prefix))\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "    # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                \n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    " # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder location: Runs/1524191220\n",
      "model location: Runs/1524191220/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# find the lastest folder for model\n",
    "\n",
    "folderList = []\n",
    "\n",
    "\n",
    "for dirs in os.listdir('Runs/'):\n",
    "    dirstring = 'Runs/' + dirs\n",
    "    if os.path.isdir(dirstring):\n",
    "        folderList.append(dirstring)\n",
    "\n",
    "folderString = folderList[len(folderList) - 1]\n",
    "folderString = folderString\n",
    "checkpointString = folderString + '/checkpoints' \n",
    "print('folder location: ' + folderString)\n",
    "print('model location: ' + checkpointString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "pre_datapath = './Predict/'\n",
    "\n",
    "# Eval Parameters\n",
    "batch_size = 64\n",
    "checkpoint_dir = folderString\n",
    "eval_train = False\n",
    "checkpoint_path = './' + checkpointString\n",
    "\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Category, Data loaded!\n",
      "Predict data loaded!\n"
     ]
    }
   ],
   "source": [
    "if eval_train:\n",
    "    x_raw = [\"a masterpiece four years in the making\", \"everything is off.\"]\n",
    "    y_test = [1, 0]\n",
    "else:\n",
    "    #x_raw, y_test = cnn_model.load_data_and_labels5(FLAGS.path)\n",
    "\t#if you are using load_data_and_labels3 in train_model, use it here as well\n",
    "    x_raw, y_test = load_data_and_labels5(pre_datapath)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print('Predict data loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map data into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jiananwen/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:203: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_path = os.path.join(checkpoint_dir, \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Runs/1524191220/checkpoints/model-1200\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_path)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(list(x_test), batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test examples: 505\n",
      "Accuracy: 0.891089\n"
     ]
    }
   ],
   "source": [
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the evaluation result to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation to Runs/1524191220/../prediction.csv\n",
      "Summary path is: Runs/1524191220/summaries\n"
     ]
    }
   ],
   "source": [
    "# decode y_test and all_predictions\n",
    "text_label = []\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0:\n",
    "        text_label.append('steam')\n",
    "    if y_test[i] == 1:\n",
    "        text_label.append('nytimes')\n",
    "    if y_test[i] == 2:\n",
    "        text_label.append('NASA')\n",
    "    if y_test[i] == 3:\n",
    "        text_label.append('linkedin')\n",
    "    if y_test[i] == 4:\n",
    "        text_label.append('bbcnews')\n",
    "\n",
    "prediction_label = []\n",
    "for i in range(len(all_predictions)):\n",
    "    if all_predictions[i] == 0:\n",
    "        prediction_label.append('steam')\n",
    "    if all_predictions[i] == 1:\n",
    "        prediction_label.append('nytimes')\n",
    "    if all_predictions[i] == 2:\n",
    "        prediction_label.append('NASA')\n",
    "    if all_predictions[i] == 3:\n",
    "        prediction_label.append('linkedin')\n",
    "    if all_predictions[i] == 4:\n",
    "        prediction_label.append('bbcnews')\n",
    "\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), prediction_label, text_label))\n",
    "out_path = os.path.join(checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "summary_path = os.path.join(checkpoint_dir, 'summaries')\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)\n",
    "    \n",
    "print('Summary path is: ' + summary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use below command to see visual summary:\n",
    "\n",
    "!tensorboard --logdir summary_path\n",
    "\n",
    "i.e: !tensorboard --logdir Runs/1524166522/summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dataset, we have 5 official account, total post number is 18293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epchos, 1290 steps, the best accuracy in validation setis 89.10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='Picture/r1.jpeg'></td><td><img src='Picture/r2.jpeg'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for test set, the accuracy is: 89.10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Britz, D. (2015, December 11). Implementing a CNN for Text Classification in TensorFlow. Retrieved from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/  \n",
    "2.Britz, D. (2015, November 7). Understanding Convolutional Neural Networks for NLP. Retrieved from http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/  \n",
    "3.Kim, J. (2017, December 2). Understanding how Convolutional Neural Network (CNN) perform text classification with word embeddings. Retrieved from https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b  \n",
    "4.A Beginner's Guide to Deep Convolutional Neural Networks (CNNs). (n.d.). Retrieved from https://deeplearning4j.org/convolutionalnetwork.html  \n",
    "5.Word2Vec word embedding tutorial in Python and TensorFlow. (2017, July 21). Retrieved from http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/  \n",
    "6.Python TensorFlow Tutorial – Build a Neural Network. (2017, April 8). Retrieved from http://adventuresinmachinelearning.com/python-tensorflow-tutorial/    \n",
    "7.Gaussic. (2018, February 19). CNN-RNN中文文本分类，基于tensorflow. Retrieved from https://github.com/gaussic/text-classification-cnn-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook and all related works are under MIT License: https://opensource.org/licenses/MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "name": "FinalProject (2)",
  "notebookId": 2694602243301778
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
